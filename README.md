# GCP
Pytorch code for GCP Network. 


Vision Transformers (ViTs), which use the transformer architecture, divide an image into a sequence of patches and extract global features through a self-attention mechanism, achieving competitive results in large-scale image classification. However, when training from scratch on small data sets, there is still a considerable gap in performance between Vision Transformers (ViTs) and Convolutional Neural Networks (CNNs), attributed to the lack of inductive bias. Since objects are locally compact and tokens are relevant, fine-grained features need to be extracted from a token and its surrounding area. Nevertheless, the lack of data hinders ViTs to learn inter-patch relations in the spatial dimension. Consequently, we propose a vision transformer based on Graph Convolutional Projection (abbreviated as GCP). This introduces graph convolution operations into the blocks of vision transformers, compelling the model to capture token features and their neighboring features. In multi-head self-attention, queries and keys are calculated through graph convolutional projection based on the spatial adjacent matrix, while the dot-product attention is used in another graph convolution to generate values.

